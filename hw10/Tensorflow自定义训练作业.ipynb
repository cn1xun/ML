{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 简答题",
   "id": "900a6e19bb0fa847"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. TensorFlow 是否可以简单替代 NumPy？两者之间的主要区别是什么？\n",
    "不能简单替代；NumPy 仅 CPU、无自动求导，TF 支持 GPU/TPU、可导、图优化\n",
    "2. 使用 `tf.range(10)` 和 `tf.constant(np.arange(10))` 是否会得到相同的结果？\n",
    "数值相同，但数据类型可能不同\n",
    "3. 可以通过编写函数或继承 `tf.keras.losses.Loss` 来定义自定义损失函数。两种方法分别应该在什么时候使用？\n",
    "简单无状态损失用函数，需要参数/序列化/有状态时用继承 Loss 类\n",
    "4. 可以直接在函数中定义自定义指标或采用 `tf.keras.metrics.Metric` 子类。两种方法分别应该在什么时候使用？\n",
    "函数：单批即得结果；子类：需跨批累积\n",
    "5. 什么时候应该自定义层而不是自定义模型？\n",
    "定义可复用计算块时自定义层，定义完整网络架构或训练行为时自定义模型\n",
    "6. 有哪些示例需要编写自定义训练循环？\n",
    "GANs等对抗训练、强化学习、自定义梯度处理、多模型联合训练等复杂场景\n",
    "7. 自定义 Keras 组件中可以包含任意 Python 代码，还是必须转换为 TF 函数？\n",
    "必须转换为 TF 函数。任意 Python 代码可能导致图构建失败或性能下降\n",
    "8. 如果要将函数转换为 TF 函数，应避免哪些主要模式？\n",
    "避免使用 Python方法、内部创建变量、使用可变数据结构输入、依赖会改变的全局变量。\n",
    "9. 何时需要创建动态 Keras 模型？ 如何动态创建Keras模型？为什么不是所有模型都动态化？\n",
    "当模型结构需在运行时确定时（如 RNNs、树状网络）。通过继承 Model 类并在 call 方法中使用控制流创建。不都动态化是因为静态图性能更好、更易部署和可视化。"
   ],
   "id": "f0568883493d9ef7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 编程题",
   "id": "9a282b6d9adda052"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. 实现一个执行层归一化的自定义层：\n",
    "    - a. `build()` 方法应定义两个可训练的权重 α 和 β，它们的形状均为 `input_shape[-1:]`，数据类型为 `tf.float32`。α 应该用 1 初始化，而 β 必须用 0 初始化。\n",
    "    - b. `call()` 方法应计算每个实例特征的均值和标准差。为此，可以使用 `tf.nn.moments(inputs, axes=-1, keepdims=True)`，它返回同一实例的均值 μ 和方差 σ²（计算方差的平方根便可获得标准差）。然后，该函数应计算并返回\n",
    "      $$\n",
    "      \\alpha \\otimes \\frac{(X-\\mu)}{(\\sigma+\\epsilon)} + \\beta\n",
    "      $$\n",
    "      其中 ε 是表示项精度的一个常量（避免被零除的小常数，例如 0.001）,$\\otimes$表示逐个元素相乘\n",
    "    - c. 确保自定义层产生与tf.keras.layers.LayerNormalization层相同（或几乎相同）的输出。\n",
    "\n",
    "2. 使用自定义训练循环训练模型来处理Fashion MNIST数据集（13_神经网络介绍 里用的数据集）：\n",
    "\n",
    "    - a.显示每个轮次、迭代、平均训练损失和每个轮次的平均精度（在每次迭代中更新），以及每个轮次结束时的验证损失和精度。\n",
    "    - b.尝试对上面的层和下面的层使用具有不同学习率的不同优化器。"
   ],
   "id": "5cd3d7096f87afd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T07:14:05.422465Z",
     "start_time": "2025-09-13T07:13:53.706186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class LayerNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, eps=1e-3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dim = input_shape[-1]\n",
    "        self.alpha = self.add_weight(\n",
    "            name='alpha', shape=(dim,), dtype=tf.float32,\n",
    "            initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(\n",
    "            name='beta', shape=(dim,), dtype=tf.float32,\n",
    "            initializer='zeros', trainable=True)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        mean, var = tf.nn.moments(inputs, axes=-1, keepdims=True)\n",
    "        std = tf.sqrt(var + self.eps)\n",
    "        return self.alpha * (inputs - mean) / std + self.beta"
   ],
   "id": "4cb9da9f5e9ea249",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T07:14:22.820004Z",
     "start_time": "2025-09-13T07:14:05.440171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# 数据\n",
    "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train, x_val = x_train/255., x_val/255.\n",
    "\n",
    "train_ds = (tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            .shuffle(60000).batch(128))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(128)\n",
    "\n",
    "# 模型（用刚才的 LayerNorm）\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128),\n",
    "    LayerNorm(),        # 自定义层\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(64),\n",
    "    LayerNorm(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "# 不同层不同优化器/学习率\n",
    "opt1 = tf.keras.optimizers.Adam(1e-3)      # 前面层\n",
    "opt2 = tf.keras.optimizers.SGD(1e-1)       # 最后层\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_loss = tf.keras.metrics.Mean()\n",
    "val_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss = loss_fn(y, logits)\n",
    "    # 分层取变量\n",
    "    vars1 = model.layers[2].trainable_variables + model.layers[0].trainable_variables # alpha beta\n",
    "    vars2 = model.layers[-1].trainable_variables # kernel b\n",
    "    grads = tape.gradient(loss, vars1 + vars2)\n",
    "    grads1, grads2 = grads[:len(vars1)], grads[len(vars1):]\n",
    "    opt1.apply_gradients(zip(grads1, vars1))\n",
    "    opt2.apply_gradients(zip(grads2, vars2))\n",
    "    train_loss.update_state(loss)\n",
    "    train_acc.update_state(y, logits)\n",
    "\n",
    "@tf.function\n",
    "def val_step(x, y):\n",
    "    logits = model(x, training=False)\n",
    "    val_loss.update_state(loss_fn(y, logits))\n",
    "    val_acc.update_state(y, logits)\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss.reset_states(); train_acc.reset_states()\n",
    "    val_loss.reset_states(); val_acc.reset_states()\n",
    "    for it, (x, y) in enumerate(train_ds, 1):\n",
    "        train_step(x, y)\n",
    "        print(f'E{epoch} Iter{it}  loss={train_loss.result():.4f}  acc={train_acc.result():.4f}', end='\\r')\n",
    "    for x, y in val_ds:\n",
    "        val_step(x, y)\n",
    "    print(f'E{epoch}  train_loss={train_loss.result():.4f}  train_acc={train_acc.result():.4f}  '\n",
    "          f'val_loss={val_loss.result():.4f}  val_acc={val_acc.result():.4f}')"
   ],
   "id": "f904ca7066171a99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1  train_loss=1.0711  train_acc=0.6472  val_loss=0.8191  val_acc=0.7030\n",
      "E2  train_loss=0.7469  train_acc=0.7355  val_loss=0.7300  val_acc=0.7329\n",
      "E3  train_loss=0.6889  train_acc=0.7531  val_loss=0.6913  val_acc=0.7480\n",
      "E4  train_loss=0.6591  train_acc=0.7625  val_loss=0.6693  val_acc=0.7578\n",
      "E5  train_loss=0.6387  train_acc=0.7695  val_loss=0.6526  val_acc=0.7649\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ec2e402d5fcb5e18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
